name: Scrape AI Releases

on:
  # Run every 6 hours
  schedule:
    - cron: '0 */6 * * *'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:

jobs:
  scrape-blogs:
    name: Scrape Official Blogs
    runs-on: ubuntu-latest
    steps:
      - name: Trigger blogs scraper
        run: |
          response=$(curl -s -w "\n%{http_code}" -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            "${{ secrets.APP_URL }}/api/scrape/blogs")

          http_code=$(echo "$response" | tail -n1)
          body=$(echo "$response" | sed '$d')

          echo "Response: $body"
          echo "HTTP Code: $http_code"

          if [ "$http_code" -ne 200 ]; then
            echo "Failed to scrape blogs"
            exit 1
          fi

  scrape-reddit:
    name: Scrape Reddit Reviews
    runs-on: ubuntu-latest
    needs: scrape-blogs  # Run after blogs scraper completes
    steps:
      - name: Trigger Reddit scraper (batch 1)
        run: |
          curl -s -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            "${{ secrets.APP_URL }}/api/scrape/reddit?limit=3&offset=0"

          # Wait to respect rate limits
          sleep 10

      - name: Trigger Reddit scraper (batch 2)
        run: |
          curl -s -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            "${{ secrets.APP_URL }}/api/scrape/reddit?limit=3&offset=3"

          sleep 10

      - name: Trigger Reddit scraper (batch 3)
        run: |
          curl -s -X POST \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            "${{ secrets.APP_URL }}/api/scrape/reddit?limit=3&offset=6"
